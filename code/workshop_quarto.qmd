---
title: "Summer at Census - Spatial Analysis Workshop"
author: 
- "Corey S. Sparks, Ph.D."
institute:
-  "Univerity of Texas at San Antonio - Department of Demography"
-  https://hcap.utsa.edu/demography
date: "July 11, 2022"
subtitle: Summer at Census Research Seminar

bibliography: SpatialDemographyClass.bib
---

## Spatial analysis workshop 
This short workshop will cover fundamentals of spatial data analysis, spatial clustering and the use of regression models that explicitly model spatial correlation in rates. I will use the Census Low Response Score as an example outcome and introduce the use of the Bayesian modeling package INLA, along with other R functions for spatial analysis. All materials will be provided on Github at the end of the workshop.
 
Data and code for all examples can be found at:

 - [Code:(https://github.com/coreysparks/summer_census)](https://github.com/coreysparks/summer_census)
 
 - [Data:(https://github.com/coreysparks/data)](https://github.com/coreysparks/data)

Additional content on spatial analysis in demography from my formal courses can be found here:

- [GIS for population science course](https://github.com/coreysparks/DEM7093)
- [Spatial demography course](https://github.com/coreysparks/DEM7263)

I also recommend using GeoDa as a tool to explore spatial data. It can be found on the [Github page for the project](https://geodacenter.github.io/). 

### Exploratory Spatial Data Analysis (ESDA)

### Skepticism and openness

In exploratory spatial data analysis (ESDA), we are looking for trends and patterns in data. We are working under the assumption that the more one knows about the data, the more effectively it may be used to develop, test and refine theory. 

This generally requires we follow two principles:

**Skepticism**: One should be skeptical of measures that summarize data, since they can conceal or misrepresent the most informative aspects of data.

**Openness**: We must be open to patterns in the data that we did not expect to find, because these can often be the most revealing outcomes of the analysis.

### Global and Local Statistics

By global we imply that one statistic is used to adequately summarize the data, i.e. the mean or median Or, a regression model that is suitable for all areas in the data Local statistics are useful when the process you are studying *varies over space*, i.e. different areas have different local values that might cluster together to form a local deviation from the overall mean Or a regression model that accounts for variation in the variables over space. This we often call *spatial heterogeneity*, meaning the process that generates the data varies over space. This is also called a *spatial regime*.

### Stationarity

Stationarity simply means that the process is not changing with respect to either time (i.e. time series analysis) or space.

This implies that the process that has generated our data is acting the same way in all areas under study.

The implications of Stationarity are that we can use a global statistic to measure our process and not feel too bad about it. It also implies that our observations are iid (independent and identically distributed) with respect to one another e.g. the parameters estimated by the regression of X on Y are the same throughout our area of study, and do not have a tendency to change. Also, it means the model estimated is equally well specified at all locations. *This is our general assumption in regression models*

### Non-Stationarity

If a process is *non-stationary* then the process changes with respect to time or space.

This implies that the process that has generated our data is not acting the same way in all areas, or the expected value (mean, or variance) of our data are subject to spatial fluctuations.

If our data are subject to such fluctuations, the this implies that our global statistics are also subject to major local fluctuations Meaning areas in our data can tend to cluster together and have similar values

### Autocorrelation

This can occur in either space or time Really boils down to the non-independence between neighboring values The values of our independent variable (or our dependent variables) may be similar because Our values occur closely in time (temporal autocorrelation) closely in space (spatial autocorrelation)

### Assessment of Spatial Dependency

Before we can explore the dependency in spatial data, we must first cover the ideas of creating and modeling neighborhoods in our data. By neighborhoods, we are referring to the clustering or connectedness of observations.

The exploratory methods we will cover in this workshop depend on us knowing how our data are arranged in space, who is next to who. This is important (as we will see later) because most correlation in spatial data tends to die out as we get further away from a specific location.

### Tobler's law

Waldo Tobler [@Tobler1970] suggested the first law of geography: - Everything is related to everything else, but near things are more related than distant things.

We can see this better in graphical form: We expect the correlation between the attributes of two points to diminish as the distance between them grows.

![](spatcorr.png)

An example of this type of phenomena is how rich and poor neighborhoods tend to cluster around one another.

So our statistics that correct for, or measure spatial association, we have to account for where observations are with respect to each other. This is typically done by specifying/identifying the spatial connectivity between observations. 

**Spatial connectivity** is defined based on the interactions/associations between features in our data.

This connectivity is often in terms of the spatial weight of an observation, in other words how much of the value of a surrounding observation do we consider when we are looking at spatial correlation.

Typically the weight of a neighboring observation decreases the further it is away from our feature of interest.

There are two typical ways in which we measure spatial relationships - distance and polygon contiguity

## Distance based association

In a distance based connectivity method, features (generally points) are considered to be contiguous if they are within a given radius of another point. The radius is really left up to the researcher to decide. For example we did this in the point analysis lab, where we selected roads within a mile of hospitals. We can equally do it to search for other hospitals within a given radius of every other hospital. The would then be labeled as neighbors according to our radius rule. Likewise, we can calculate the distance matrix between a set of points This is usually measured using the standard Euclidean distance

$d^2=\sqrt{(x_1-x_2)^2 + (y_1 - y_2)^2}$

Where x and y are coordinates of the point or polygon in question (selected features), this is the as the crow flies distance. *There are lots of distances*

### Spatial Neighbors

There are many different criteria for deciding if two observations are neighbors Generally two observations must be within a critical distance, $d$, to be considered neighbors. This is the Minimum distance criteria, and is very popular. This will generate a matrix of binary indicators describing the neighborhood. We can also describe the neighborhoods in a continuous weighting scheme based on the distance between them

*Inverse Distance Weight*

$w_{ij} = \frac{1}{d_{ij}}$

or *Inverse-Squared Distance Weight*

$w_{ij} = \frac{1}{d_{ij}^2}$

*K nearest neighbors* A useful way to use distances is to construct a k-nearest neighbors set. This will find the "k" closest observations for each observation, where k is some integer.

For instance if we find the k=3 nearest neighbors, then each observation will have 3 neighbors, which are the closest observations to it, *regardless of the distance between them* which is important. 

Using the k nearest neighbor rule, two observations could potentially be very far apart and still be considered neighbors.

If the a feature is within the *threshold distance*, a 1 is given, otherwise a 0 is given. Neighborhoods are created based on which observations are judged contiguous.


## Polygon Adjacency weights

This is generally the best way to treat polygon features Polygons are contiguous if they share common topology, like an edge (line segment) or a vertex (point). Think like a chess board: *Rook adjacency* Neighbors must share a line segment

*Queen adjacency* Neighbors must share a vertex or a line segment If polygons share these boundaries (based on the specific definition: rook or queen), they are given a weight of 1 (adjacent), else they are given a value 0, (nonadjacent)

![](adj.png)

**What does a spatial weight matrix look like?** 
Assume this is our data: 
![](poly4.png)

This would be a *Rook-based* adjacency weight matrix:

$$
w_{ij} = \begin{bmatrix}
0 & 1 & 1 & 0\\ 
1 & 0 & 0 & 1 \\ 
1 & 0 &  0& 1\\ 
 0&  1& 1 & 0
\end{bmatrix}
$$

$w_{ij}$ = 1 if polygons share a border, 0 if they don't. Also note that an observation can't be it's own neighbor and the diagonal elements of the matrix are all 0.

### Measuring Spatial Autocorrelation

If we observe data $Z(s)$ (an attribute) at location $i$, and again at location $j$, the spatial autocorrelation between $Z(s)_i$ and $Z(s)_j$ is degree of similarity between them, measured as the standardized covariance between their locations and values. 

In the absence of spatial autocorrelation the locations of $Z(s)_i$ and $Z(s)_j$ has nothing to do with the values of $Z(s)_i$ and $Z(s)_j$ although, if autocorrelation is present, close proximity of $Z(s)_i$ and $Z(s)_j$ leads to correlation of their attributes.

*Positive autocorrelation* Positive autocorrelation means that a feature is positively associated with the values of the surrounding area (as defined by the spatial weight matrix), high values occur with high values, and low with low. For example, poverty rates typically cluster together, meaning you have poor neighborhoods next to other poor neighborhoods. This results in positive auto-correlation. Similarly, you typically see that affluent neighborhoods are geographically close to other affluent neighborhoods.

Both of these are examples of positive auto-correlation.

*Negative autocorrelation* Negative autocorrelation means that a feature is negatively associated with the values of the surrounding area, high with low, low with high. Negative auto-correlation is rarer to see, but an example could be when neighborhoods are gentrifying. When gentrification occurs, you typically have one area where new home construction or new business construction happens, but it it typically surrounded by areas where development is lacking. This would then create a neighborhood of relative affluence among other neighborhoods which are struggling. This is an example of negative auto-correlation.

*Spatial lags* of a variable are done via multiplying the variable through the spatial weight matrix for the data.

If we have a value $Z(s_i)$ at location i and a spatial weight matrix $w_{ij}$ describing the spatial neighborhood around location i, we can find the lagged value of the variable by: $WZ_i = Z(s_i) * w_{ij}$

This calculates what is effectively, the neighborhood average value in locations around location i, often stated $Z(s_{-i})$

Let's return to the adjacency matrix from above, a *Rook-based* adjacency weight matrix.

$$
w_{ij} = \begin{bmatrix}
0 & 1 & 1 & 0\\ 
1 & 0 & 0 & 1 \\ 
1 & 0 &  0& 1\\ 
 0&  1& 1 & 0
\end{bmatrix}
$$

Typically this matrix is standardized, by dividing each element of $w_{ij}$ by the number of neighbors, this is called the *row-standardized* form of the matrix. In this case, there are two neighbors, so row-standardization results in the following matrix:

$$
w_{ij} = \begin{bmatrix}
0 & .5 & .5 & 0\\ 
.5 & 0 & 0 & .5 \\ 
.5 & 0 &  0& .5\\ 
 0&  .5& .5 & 0
\end{bmatrix}
$$

Let's take a variable z, equal to:

$$
z=\begin{bmatrix}
1 & 5 & 10 & 2
\end{bmatrix}
$$

When we form the product: $z'W$, we get:

$$
z_{lag}=\begin{bmatrix}
7.5 & 1.5 & 1.5 & 7.5
\end{bmatrix}
$$

In R, we can calculate the spatially lagged value of z. See the code below.

```{r}
z<-c(1,5,10,2)
w<-matrix(c(0,.5,.5,0,.5,0,0,.5,.5,0,0,.5,0,.5,.5,0), nrow = 4, byrow = T)
z
```

```{r}
w
```

```{r}
z%*%w
```

### Measureing spatial autocorrelation

### Moran's I Statistic

One of the most popular global auto-correlation statistic is Moran's I [@Moran1950]

$I = \frac{n}{(n - 1)\sigma^2 w_{..}} \sum^i_n \sum^j_n w_{ij} (Z(s_i) - \bar Z)(Z(s_j) - \bar Z)$

with:

\ -$Z(s_i)$ being the value of the variable, the variable for example, at location i

\ -$Z(s_j)$ being the value of the variable at location j,

\ -$\sigma^2$ is sample variance of the variable

\ -$w_{ij}$ is the weight for location *ij* (0 if they are not neighbors, 1 otherwise).

Moran's I is basically a correlation, *think of a Pearson correlation* $\rho$, *between a variable and a spatially lagged version of itself*.

**Moran's I Scatter plot** It is sometimes useful to visualize the relationship between the actual values of a variable and its spatially lagged values. This is the so called **Moran scatter plot**

Lagged values are the average value of the surrounding neighborhood around location $i$

lag(Z) = $z_{ij} * w_{ij}$ = $z'W$ in matrix terms

Which, now we see where we get the *y-value* of the Moran scatter plot. It is just the lagged version of the original variable.

The Moran scatter plot shows the association between the value of a variable in a location and its spatial neighborhood's average value. The variables are generally plotted as *z-scores*,to avoid scaling issues.

And here we show the Moran scatter plot. This plot shows the z-scored values of the original variable on the x axis and the spatially lagged values of the original variable on the y axis. In this case, we see a weak positive relationship to the values. This positive relationship indicates positive auto-correlation in this variable.

**Moran Scatter plot for San Antonio Low Response Rate**

```{r}

library(tidyverse)

tx_lrs <- sf::st_read("../data/tx_lrs.gpkg")

sa_lrs <- tx_lrs %>%
  filter(COUNTYFP == "029")

```

Here is the overall low response rate map for San Antonio.

```{r}
library(tmap)

tmap_mode("view")

tm_shape(sa_lrs)+
  tm_polygons("Low_Respon",
                style="quantile",
               n=5,
               legend.hist = TRUE) +
  tm_basemap(server="OpenStreetMap",alpha=0.5)
```

### Create spatial adjacency information

```{r,echo=TRUE, warning=FALSE, message=FALSE}
library(sfdep)

#Make a rook style weight matrix
sanb <- sa_lrs %>%
  mutate(nb = st_contiguity(geom, queen = F), 
         wt = st_weights(nb),
         .before = 1)
```

```{r}
global_moran_perm(sanb$Low_Respon,
             nb = sanb$nb,
             wt = sanb$wt, 
             nsim = 1000)

```

Moran's I is basically a correlation think Pearson's $\rho$ on a variable and a *spatially lagged* version of itself. Spatial lags of a variable are done via multiplying the variable through the spatial weight matrix for the data.

### Local Moran's I

So far, we have only seen a *Global* statistic for auto-correlation, and this tells us if there is any *overall* clustering in our data. We may be more interested in precisely *where* within the data the auto-correlation occurs, or where *clusters* are located.

A local version of the Moran statistic is available as well. This basically calculates the Moran I statistic from above, but only for each observation's *local neighborhood*.

It compares the observation's value to the local neighborhood average, instead of the global average. Luc Anselin [@Anselin2010a] referred to this as a "**LISA**" statistic, for *Local Indicator of Spatial Autocorrelation*.

Here is a LISA map for clusters of the low response score, which shows areas of concentrated (clustered) high LRS clustering in red, and concentrated low values of the LRS in blue.

```{r}

sa_locali <- sa_lrs%>%
  transmute(lisa = local_moran(Low_Respon,
                                nb = sanb$nb,
                                wt = sanb$wt,
                                nsim = 1000))%>%
               tidyr::unnest(lisa)

sa_locali <- sa_locali %>%
  mutate(cluster = ifelse(p_ii_sim < .05,
                          as.character(mean),
                          NA)) 
```

```{r}
tmap_mode("view")

tm_shape(sa_locali)+
  tm_polygons("cluster",
              palette = "RdBu",
              textNA = "Not Significant")+
  tm_basemap(server="OpenStreetMap",alpha=0.5)

```

The red and blue areas are so-called *clusters*, because they are areas with higher (or lower, for the blues) than average LRS values, surrounded by areas that also have higher than average LRS values. The red clusters are so called "*high-high clusters*", likewise the blue areas are called "*low-low clusters*".

We also see in the legend that light pink and light blue values are possible. The light pink polygons represent areas that have high values of LRS, but are in a LRS spatial neighborhood, and are called high-low *outliers*. These are rare in this example.

Likewise, possible values include light blue polygons, these are called low-high outliers, because they have low LRS scores, but are in a high LRS spatial neighborhood. These are also rare in this example.

There are of course other autocorrelation statistics. For instance Geary's C

### Geary's C

-   RC Geary in [1954](http://www.jstor.org/stable/2986645) derived the C statistic

-   $C = \frac{n-1}{2 \sum_{ij} w_{ij}} \frac{\sum_{ij} w_{ij} \left ( x_i - x_j \right )^2 }{\sum_{ij} \left ( x_i - \bar x \right )^2 }$

-   Similar in interpretation to the Moran statistic, C, measures whether values are similar in neighboring areas.

-   C == 1 == No autocorrelation, C\< 1 == positive autocorrelation, C \> 1 negative autocorrelation

### Getis-Ord G

-   `"{Too Ugly to Show}"` [See the paper](http://onlinelibrary.wiley.com/store/10.1111/j.1538-4632.1992.tb00261.x/asset/j.1538-4632.1992.tb00261.x.pdf?v=1&t=it0w4k1t&s=a164f95f2fd2c46259b70d859f2366f1e8cbae2d)

-   Similar to Geary's C in interpretation

-   High values next to high values, and so on

### What these methods tell you?

-   Moran's I is a *descriptive statistic ONLY*,
-   It simply indicates if there is spatial association/autocorrelation in a variable
-   Local Moran's I tells you if there is significant localized clustering of the variable


#  - Break - 

## Regression modeling for spatial data using INLA

## The INLA Approach to Bayesian models
The Integrated Nested Laplace Approximation, or INLA, approach is a recently developed, computationally simpler method for fitting Bayesian models [(Rue et al., [2009](http://onlinelibrary.wiley.com/store/10.1111/j.1467-9868.2008.00700.x/asset/j.1467-9868.2008.00700.x.pdf?v=1&t=ih5b86ev&s=9078c3b0adb48d4c15bc49ae3ededc6d1cd684c5), compared to traditional Markov Chain Monte Carlo (MCMC) approaches. INLA fits models that are classified as latent Gaussian models, which are applicable in many settings (Martino & Rue, [2010](http://www.bias-project.org.uk/gmrfcourse/inla-program.pdf).  In general, INLA fits a general form of additive models such as:

$\eta = \alpha + \sum_{j=1}^{nf} f^{(j)}(u_{ij}) + \sum_{k=1}^{n\beta}\beta_k z_{ki} + \epsilon_i$

where $\eta$ is the linear predictor for a generalized linear model formula, and is composed of a linear function of some variables u, $\beta$ are the effects  of covariates, z, and $\epsilon$ is an unstructured residual (Rue et al., 2009). As this model is often parameterized as a Bayesian one, we are interested in the posterior marginal distributions of all the model parameters. Rue and Martino [(2007)](http://www.sciencedirect.com/science/article/pii/S0378375807000845) show that the posterior marginal for the random effects (x) in such models can be approximated as:

$\tilde{p}(x_i|y) = \sum_k \tilde{p}(x_i|\theta_k, y) \tilde{p}(\theta_k|y) \Delta_k$

via numerical integration (Rue & Martino, 2007; Schrodle & Held, [2011a](http://onlinelibrary.wiley.com/doi/10.1002/env.1065/full), [2011b](http://link.springer.com/article/10.1007/s00180-010-0208-2)). The posterior distribution of the hyperparameters ($\theta$) of the model can also be approximated as:

$\tilde{p}(\theta | y)) \propto \frac{p(x, \theta, y)}{\tilde{p}G(x| \theta,y)} \mid _{x} = x^*(\theta)$

, where G is a Gaussian approximation of the posterior and $x^*(\theta)$ is the mode of the conditional distribution of $p(x|\theta,y)$. Thus, instead of using MCMC to find an iterative, sampling-based estimate of the posterior, it is arrived at numerically. This method of fitting the spatial models specified above has been presented by numerous authors (Blangiardo & Cameletti, [2015](https://books.google.com/books?hl=en&lr=&id=--HuBgAAQBAJ&oi=fnd&pg=PA259&dq=Blangiardo+%26+Cameletti,+2015&ots=VSDEJ7wfM2&sig=graudrEKTevK2HR7nozmZ-Y5N0Q#v=onepage&q=Blangiardo%20%26%20Cameletti%2C%202015&f=false); Blangiardo et al., [2013](http://www.sciencedirect.com/science/article/pii/S1877584513000336); Lindgren & Rue, [2015](http://www.sciencedirect.com/science/article/pii/S2211675315000780); Martins et al., [2013](http://www.sciencedirect.com/science/article/pii/S0167947313001552); Schrodle & Held, 2011a, 2011b), with comparable results to MCMC.

### Libraries

You need to install INLA, if you're using R >= 4.1, you install via

`install.packages("INLA",repos=c(getOption("repos"),INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)`

```{r libraries, results='hide', message=FALSE, warning=FALSE}
#library(rgdal)
library(spdep)
library(INLA)
library(tigris)
library(tidycensus)
library(tidyverse)

```

### Data
I have the data on my github site under the [nhgis0029_csv](https://github.com/coreysparks/data/tree/master/nhgis0029_csv) page. These are data from the [NHGIS](https://www.nhgis.org/) project by [IPUMS](https://www.ipums.org/) who started providing birth and death data from the US Vital statistics program. 

The data we will use here are infant mortality rates in US counties between 2001 and 2007. 

```{r load data, message=FALSE, warning=FALSE}
files<-list.files("C:/Users/ozd504/Github//data/nhgis0029_csv/",
                  pattern = "*.csv",
                  full.names = T)
df<-read_csv(files[16])
df$cofips<-paste(substr(df$GISJOIN, 2,3),
                 substr(df$GISJOIN, 5,7),
                 sep="")

df<-df%>%
  mutate(deaths=as.numeric(AGWI001),
         births = as.numeric(AGWE001))%>%
  arrange(cofips)%>%
  select(cofips,deaths,births)

head(df)

```


## Get census data using tidycensus
Here I get data from the 2000 decennial census summary file 3
```{r}
#v00<-load_variables(year=2000, dataset = "sf3", cache = T)
cov_dat<-get_decennial(geography = "county",
                       year = 2000,
                       sumfile = "sf3",
                       summary_var = "P001001",
                       variables = c("P007003", "P007004","P007010","P053001", "P089001", "P089002" ),
                      output = "wide")

cov_dat<-cov_dat%>%
  mutate(cofips=GEOID,
         pwhite=P007003/summary_value,
         pblack=P007004/summary_value,
         phisp=P007010/summary_value,
         medhhinc=as.numeric(scale(P053001)),
         ppov=P089002/P089001)


final.dat<-merge(df, cov_dat, by="cofips")
head(final.dat)

```


## Create expected numbers of cases
In count data models, and spatial epidemiology, we have to express the raw counts of events relative to some expected value, or population offset, see [this Rpub](http://rpubs.com/corey_sparks/361894) for a reminder.

```{r}
final.dat$E_d<-final.dat$births * (sum(final.dat$deaths)/sum(final.dat$births))

final.dat <- final.dat[order(final.dat$cofips),]
final.dat$id <- 1:dim(final.dat)[1]

head(final.dat)
options(scipen=999)
```

Next we make the spatial information, we get the polygons from census directly using `counties` from the `tigris` package. We drop counties not in the contiguous 48 US states. 

```{r, results='hide'}
library(tigris)
us_co<-counties( cb = T)
us_co<-us_co%>%
  subset(!STATEFP%in%c("02", "15", "60", "66", "69", "72", "78"))

```

## Construction of spatial relationships among counties

Key part here is to make numeric identifier for each geography!

```{r}
#In INLA, we don't need FIPS codes, we need a simple numeric index for our counties
us_co$struct<-1:dim(us_co)[1]  

nbs<-knearneigh(st_centroid(us_co), k = 5, longlat = T) #k=5 nearest neighbors

nbs<-knn2nb(nbs, row.names = us_co$struct, sym = T) #force symmetry!!

mat <- nb2mat(nbs, style="B",zero.policy=TRUE)

colnames(mat) <- rownames(mat) 

mat <- as.matrix(mat[1:dim(mat)[1], 1:dim(mat)[1]])


nb2INLA("cl_graph",nbs)
am_adj <-paste(getwd(),"/cl_graph",sep="")
H<-inla.read.graph(filename="cl_graph")
```

### Plot geographies

```{r}
library(sf)
us_co<-st_as_sf(us_co)
us_co$cofips<-paste(us_co$STATEFP, us_co$COUNTYFP, sep="")

us_co%>%
  ggplot()+
  geom_sf()+
  coord_sf(crs = 2163)
```

```{r}
final.dat<-merge( us_co,final.dat, by="cofips")
final.dat<-final.dat[order(final.dat$cofips),]
```



```{r, results='hide', echo=FALSE}
sts<- states(cb=T)%>%
  st_transform(crs= 2163)%>%
  st_boundary()%>%
  subset(!STATEFP%in%c("02", "15", "60", "66", "69", "72", "78"))
```

## Map of the Infant mortality raw relative risk

```{r}
final.dat%>%
  #filter(year%in%c(2000))%>%
  mutate(qrr=cut(I(deaths/E_d), 
                 breaks = quantile(I(deaths/E_d),
                                   p=seq(0,1,length.out = 5)),
                 include.lowest = T))%>%
  ggplot()+
  geom_sf(aes(fill=qrr, color=NA))+
  geom_sf(data=sts, color="black")+
  scale_colour_brewer(palette = "RdBu" )+
  scale_fill_brewer(palette = "RdBu", na.value="grey")+
  guides(fill=guide_legend(title="Relative Risk Quartile"))+
  ggtitle(label="Relative Risk Quartile - IMR Raw data, 2000")+
  coord_sf(crs = 2163)

#ggsave(filename = "C:/Users/ozd504/Documents/GitHub/talks/imr_raw2000.png", dpi = "print", width = 10, height = 8)
```
# Model setup
- We have a count outcome (deaths and births), in counties over time, and a set of time-constant covariates.

- We have several options in the GLM framework with which to model these data, for example:

- Binomial - $$y_{ij} \sim Bin(\pi_{ij}) \text{:  } logit(\pi_{ij} ) = \beta_{0}+ x'\beta_k $$

- Poisson - $$y_{ij} \sim Pois(\lambda_{ij} E_{ij}) \text{:  } log(\lambda_{ij} ) = log(E_{ij}) + \beta_{0}+ x'\beta_k $$

- Negative Binomial - $$y_{ij} \sim \text{Neg Bin} (\mu_{ij}, \alpha, E_{ij}) \text{:  } log(\mu_{ij} ) = log(E_{ij}) + \beta_{0}+ x'\beta_k $$
 
- In addition to various zero-inflated versions of these data.

We can fit these  model using the Bayesian framework with INLA. 

First, we consider the basic GLM for the mortality outcome, without any hierarchical structure. 

We can write this model as a Negative Binomial model, for instance as:

$\text{Deaths_ij} = log(E_d) + X' \beta $

INLA will use vague Normal priors for the $\beta$'s, and we have not other parameters in the model to specify priors for. 

INLA does not require you to specify all priors, as all parameters have a default prior specifications.

### Basic INLA model specification

```{r}
#Model specification:
f1<-deaths~scale(pblack)+scale(phisp)+scale(ppov)

#Model fit
mod1<-inla(formula = f1, #linear predictor - fixed effects
           data = final.dat, #data set object name
           family = "poisson", #marginal distribution for the outcome
           E = E_d, # expected count
           control.compute = list(waic=T), # compute DIC or not?
           control.predictor = list(link=1), #estimate predicted values & their marginals or not?
           num.threads = 3, # control number of computing cores
               verbose = F)

#model summary
summary(mod1)

```


### Basic county level random intercept model
Now we add basic nesting of rates within counties, with a random intercept term for each county. This would allow there to be heterogeneity in the mortality rate for each county, over and above each county's observed characteristics. 

This model would be:

$\text{Deaths}_{ij} = log(E_d) + X' \beta + u_j$

$u_j \sim  \text{Normal} (0 , \tau_u)$

where $\tau_u$ here is the precision, not the variance and **precision = 1/variance.** 

INLA puts a log-gamma prior on the the precision by default.

```{r}
f2<-deaths~scale(pblack) + scale(phisp) + scale(ppov)+ #fixed effects
  f(struct, model = "iid")  #random effects

mod2<-inla(formula = f2,
           data = final.dat,
           family = "poisson",
           E = E_d, 
           control.compute = list(waic=T), 
           control.predictor = list(link=1),
           num.threads = 3, 
               verbose = F)

#total model summary
summary(mod2)

```

#### Marginal Distributions of hyperparameters
We can plot the posterior marginal of the hyperparameter in this model, in this case $\sigma_u = 1/\tau_u$

```{r}
m2<- inla.tmarginal(
        function(x) (1/x), #invert the precision to be on variance scale
        mod2$marginals.hyperpar$`Precision for struct`)
#95% credible interval for the variance
inla.hpdmarginal(.95, marginal=m2)

plot(m2,
     type="l",
     main=c("Posterior distibution for between county variance", "- IID model -"), 
     xlim=c(0, .5))

```



```{r}
final.dat$fitted_m2<-mod2$summary.fitted.values$mean

final.dat%>%
  mutate(qrr=cut(fitted_m2,
                 breaks = quantile(fitted_m2, p=seq(0,1,length.out = 6)),
                 include.lowest = T))%>%
  ggplot()+geom_sf(aes(fill=qrr))+
  scale_colour_brewer(palette = "RdBu" )+
  scale_fill_brewer(palette = "RdBu", na.value="grey")+
  guides(fill=guide_legend(title="Relative Risk Quartile"))+
  ggtitle(label="Relative Risk Quartile - IID Model, 2000")+
  coord_sf(crs = 2163)
  
```

### Besag, York, and Mollie - BYM Model

Model with spatial correlation - Besag, York, and Mollie (1991) model 

$$\text{Deaths}_{ij} = log(E_d) + X' \beta + u_j + v_j$$

Which has two random effects, one an IID random effect and the second a spatially correlated random effect, specified as a  conditionally auto-regressive prior for the $v_j$'s. 

This is the Besag model:

$$v_j|v_{\neq j},\sim\text{Normal}(\frac{1}{n_i}\sum_{i\sim j}v_j,\frac{1}{n_i\tau})$$

and $u_j$ is an IID normal random effect, $\gamma_t$ is also given an IID Normal random effect specification, and there are now three hyperparameters, $\tau_u$ and $\tau_v$ and $\tau_{\gamma}$ and each are given log-gamma priors.

For the BYM model we must specify the spatial connectivity matrix in the random effect.

```{r}

f3<-deaths~scale(pblack)+scale(phisp)+scale(ppov)+
  f(struct, model = "bym",scale.model = T, constr = T, graph = H) #specify graph for spatial correlation model

mod3<-inla(formula = f3,data = final.dat,
           family = "poisson",
           E = E_d, 
           control.compute = list(waic=T, return.marginals.predictor=TRUE), 
           control.predictor = list(link=1), 
           num.threads = 3, 
               verbose = F)

#total model summary
summary(mod3)
```

Compare model fits

```{r}
mod1$waic$waic
mod2$waic$waic
mod3$waic$waic

```

Looks like the spatial is better in this case. 

#### Regression effect posterior distributions

If we want to get the 95% credible interval for a risk ratio $exp(\beta)$, we can do so using the `inla.emarginal()` and `inla.tmarginal()` functions

```{r}

b1 <- inla.emarginal(exp, mod2$marginals.fixed[[2]])
b1 #Posterior mean

b1ci <-inla.qmarginal(c(.025, .975), 
                      inla.tmarginal(exp, mod2$marginals.fixed[[2]]))
b1ci
```

And we see that the `pblack` effect increases the infant mortality risk by `r round(b1-1, 2)`%



#### Hyper parameter distributions

Green line is spatial variation, black line is non-spatial variation

```{r}

m3a<- inla.tmarginal(
        function(x) (1/x),
        mod3$marginals.hyperpar$`Precision for struct (iid component)`)

m3b<- inla.tmarginal(
        function(x) (1/x),
        mod3$marginals.hyperpar$`Precision for struct (spatial component)`)

plot(m3a, type="l",
     main=c("Posterior distibution for between county variance", "- BYM model -"),
     xlim=c(0, .2)) # you may need to change this

lines(m3b, col="green")
```

HPD interval for variances

```{r}
inla.hpdmarginal(.95,m3a)
inla.hpdmarginal(.95,m3b)


```


## Spatial mapping of the fitted values
 Here we can recover the estimated smoothed relative risks for each county, based on the model above.

```{r}
final.dat$fitted_m3<-mod3$summary.fitted.values$mean

final.dat%>%
 # filter(year%in%c(2000))%>%
  mutate(qrr=cut(fitted_m3,
                 breaks = quantile(fitted_m3, p=seq(0,1,length.out = 6)),
                 include.lowest = T))%>%
  ggplot()+
  geom_sf(aes(fill=qrr))+
  scale_colour_brewer(palette = "RdBu" )+
  scale_fill_brewer(palette = "RdBu", na.value="grey")+
  guides(fill=guide_legend(title="Relative Risk Quartile"))+
  ggtitle(label="Relative Risk Quartile - BYM Model, 2000")+
  coord_sf(crs = 2163)
  

```

## Map of spatial random effects

It is common to map the random effects from the BYM model to look for spatial trends, in this case, there is a strong spatial signal:

```{r}
us_co$sp_re<-mod3$summary.random$struct$mean[3109:6216]
us_co%>%
  mutate(qse=cut(sp_re, 
                 breaks = quantile(sp_re, p=seq(0,1,length.out = 6)),
                 include.lowest = T))%>%
  ggplot()+
  geom_sf(aes(fill=qse, color=NA))+
  geom_sf(data=sts)+
  scale_colour_brewer(palette = "RdBu" )+
  scale_fill_brewer(palette = "RdBu", na.value="grey")+
  guides(fill=guide_legend(title="Spatial Excess Risk"))+
  ggtitle(label="Spatial Random Effect - BYM Model")+
  coord_sf(crs = 2163)

```

### Spatial + IID random effect

```{r}
us_co$i_re<-mod3$summary.random$struct$mean[1:3108]
us_co%>%
  mutate(qse=cut(i_re, 
                 breaks = quantile(i_re, p=seq(0,1,length.out = 6)),
                 include.lowest = T))%>%
  ggplot()+
  geom_sf(aes(fill=qse, color=NA))+
  geom_sf(data=sts)+
  scale_colour_brewer(palette = "RdBu" )+
  scale_fill_brewer(palette = "RdBu", na.value="grey")+
  guides(fill=guide_legend(title="Spatial Excess Risk"))+
  ggtitle(label="Combined Random Effect - BYM Model")+
  coord_sf(crs = 2163)

```

## Summary
 
 - Spatial analysis often involves a lot of exploratory data analysis. Maps and statistics like Moran's I are very useful to explore your data
 
 - Regression models can easily incorporate spatial correlation structures, and the INLA approach allows a very flexible method for estimating such models in the Bayesian framework. 
 
 
